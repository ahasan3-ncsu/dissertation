\chapter{Theoretical Background}

\section{Binary Collision Approximation}

Placeholder text

\section{Molecular Dynamics}

Molecular dynamics (MD) is a computer simulation technique
where the time evolution of a set of interacting atoms
is calculated using equations of motion.
The laws of classical mechanics, such as Newton's second law,
is followed in this technique.
For example, in a closed system consisting of $N$ atoms,
the force acting on an atom $i$ is
\begin{align}
	\bm{F}_i = m \bm{a}_i = m \frac{d^2 \bm{r}_i}{dt^2}
\end{align}

This force is a result of the interactions of atom $i$
with all the other atoms in the system.
If the force acting on an atom is known, its movement can be predicted
given the initial position and velocity \cite{ercolessi}.
We can derive these forces
by computing the gradients of the potential energy of the system
with respect to atomic displacements.
\begin{align}
	\bm{F}_i = - \nabla_{\bm{r}_i} V(\bm{r}_1,\ldots,\bm{r}_N)
\end{align}

The potential energy of the system actually depends on the interaction
between the subatomic particles in the atoms.
Thus, the true interatomic interactions are quantum mechanical in nature.
It is not known whether the interactions described by
the Schr\"odinger equation or Dirac equation for all subatomic particles
could be cast into an analytical functional form
to describe the interactions between atoms.
Thus, all descriptions of interatomic potentials are approximations.

In \textit{ab initio} MD (AIMD), interatomic forces are calculated on-the-fly
using quantum mechanicaly methods, typically density functional theory (DFT).
Forces are obtained from the electronic wavefunction
computed at each time step using DFT,
while assuming electrons adapt instantaneously compared to nuclei positions.
This is known as Born-Oppenheimer approximation.
Although calculations from first-principles are accurate,
they are computationally expensive and sometimes infeasible for certain tasks.

Finding a practical and efficient description of the potential energy,
albeit approximate, is the crux of MD.
The classical interatomic potentials try to approximate
the true nature of atomic interactions at the expense of accuracy,
the advantage being reduced computational cost \cite{marx}.
Interatomic potentials can be either parametric or non-parametric.
We will discuss the differences between them in the following section.

After defining the interactions among atoms through a potential,
new positions and velocities of atoms can be calculated using time integration.
Velocity-Verlet is the most commonly used integration scheme in MD
\cite{verlet, tuckerman}.
This allows the simulated systems to evolve in time,
which in turn provides insight into the behavior of complex atomic systems.

MD simulations are mainly done in the microcanonical ensemble.
However, in many situations,
the simulated system needs to have a constant temperature.
Among many methods introduced to control the temperature in a MD simulation,
the Nos\'e-Hoover thermostat is the most popular one \cite{hoover}.

% NOTE: should this be a section?
\section{Interatomic Potential}

% TODO: citation needed
Interatomic potentials come in different forms
to accomodate different physical motivations.
Historically, most interatomic potentials could be described as "parametric",
since they are developed with a fixed number of physical terms and parameters.
In non-parametric potentials,
the total number of terms and parameters are flexible.
This allows systematic optimization of potentials
using complex local neighbor descriptors
and separate mappings to predict different system properties.
Even though the non-parametric potentials can be more accurate,
the lack of physical meaning behind the parameters
can make justification of extrapolation and uncertainty quantification harder.

The simplest parametric interatomic potential
would only consider pairwise interactions between atoms,
using pairwise potential function $\phi_{\alpha\beta}$
between atom types $\alpha$ and $\beta$.
These two-body (pairwise) potentials are poor for metallic systems.
The embedded-atom method (EAM) overcomes this problem
by adding an embedding energy term to the pairwise potential
\cite{daw1984eam, daw1993eam}.
In EAM, the potential energy $V_i$ of an atom $i$ is given by
\begin{align}
	V_i
	= \frac{1}{2} \sum_{j \ne i} \phi_{\alpha\beta} (r_{ij})
		+ F_{\alpha} \bigg( \sum_{j \ne i} \rho_{\beta} (r_{ij}) \bigg)
\end{align}
where $r_{ij}$ is the distance
between atom $i$ of type $\alpha$ and neighbors $j$ of type $\beta$,
$\rho_{\beta}$ is the contribution to the electro charge density
from atom $j$ at the location of atom $i$,
and $F_{\alpha}$ is an embedding function that represents
the energy required to place atom $i$ into the electron cloud.

% TODO: citation needed
The non-parametric potentials are often referred to
as machine-learned interatomic potentials (MLIPs).
While non-parametric models are especially suitable for the application of ML,
it is to be noted that
parametric potentials can also be optimized using machine learning,
The potential energy of a system in an MLIP is written as
\begin{align}
	V_{tot}
	= \sum_i V_i (\bm{q}_i)
\end{align}
where $\bm{q}_i$ is a mathematical representation
of the atomic environment surrounding the atom $i$,
commonly known as the descriptor.
$V_i$ is a machine-learning model that provides energy prediction
based on the descriptor.
An MLIP requires a robust descriptor
that maintains certain physical symmetries,
and a suitable machine learning framework for the descriptor.

\subsection{Angular Dependent Potential}

% TODO: why ADP?

The angular dependent potential (ADP) formalism
is a generalization of the EAM type potential
\cite{mishin2005adp, beelerUMoXe}.
The potential energy $V_i$ of an atom $i$ in ADP is given by
\begin{align}
	V_i
	&= \frac{1}{2} \sum_{j \ne i} \phi_{\alpha\beta} (r_{ij})
		+ F_{\alpha} \bigg( \sum_{j \ne i} \rho_{\beta} (r_{ij}) \bigg)
		+ \frac{1}{2} \sum_{s} (\mu_i^s)^2
		+ \frac{1}{2} \sum_{s, t} (\lambda_i^{st})^2
		- \frac{1}{6} \sum_i \nu_i^2 \\
	\mu_i^s
	&= \sum_{j \ne i} u_{\alpha\beta} (r_{ij}) r_{ij}^s \\
	\lambda_i^{st}
	&= \sum_{j \ne i} w_{\alpha\beta} (r_{ij}) r_{ij}^s r_{ij}^t \\
	\nu_i
	&= \sum_s \lambda_i^{ss}
\end{align}
where $i$, $j$, $\alpha$, $\beta$, $\phi_{\alpha\beta}$, and $F_{\alpha}$
have the same definitions as in the EAM formalism.
Additionally, $s$ and $t = 1, 2, 3$ and refer to the Cartesian coordinates.
The $\mu$ and $\lambda$ terms represent dipole and quadrupole distortions
of the local atomic environment.
This formalism extends the EAM by introducing angular forces
as dipole and quadrupole moments.

\section{Bayesian Statistics}

% TODO: citation needed
The probability of two events $A$ and $B$ happening together
can be expressed as:
\begin{align}
	P(A \cap B) &= P(B) P(A|B) = P(A) P(B|A)
\end{align}
where $P(B)$ means the probability of $B$ occurring,
and $P(A|B)$ denotes the probability of
$A$ occurring given $B$ already occurred.
Put simply, $P(A|B)$ is called
the \textit{conditional} probability of $A$ given $B$.
$P(A)$ and $P(B|A)$ have similar meanings.
Rewriting the above equation leads to:
\begin{align}
	P(A|B) &= \frac{P(B|A)P(A)}{P(B)}
\end{align}
where $P(B) \ne 0$.
This is known as the Bayes' theorem.

Bayesian statistics is a subfield of statistics that utilizes
a particular interpretation of probability based on the Bayes' theorem.
In this interpretation, probability signifies a degree of belief in an event.
The degree of belief may depend on prior knowledge about the event,
such as previous experimental results or personal beliefs.
Specifically, the Bayesian interpretation has a way of
incorporating prior knowledge in the form of probability distributions.
In contrast, the frequentist interpretation views probability as
the limit of the relative frequency of an event after many trials.

To expand on the Bayesian interpretation, let's assume
$\bm{Y} = (Y_1, \ldots, Y_n)$ is the data produced by a random process
and the random process can be modeled in terms of fixed but unknown parameters
$\bm{\theta} = (\theta_1, \ldots, \theta_p)$.
Then, the probability distribution $\mathcal{L}(\bm{Y} | \bm{\theta})$
indicates the \textit{likelihood} of $\bm{Y}$ given parameters $\bm{\theta}$.
Bayesian inference is concerned with the inverse problem of
estimating $\bm{\theta}$ using the likelihood function.
Since almost all data have noise, $\bm{\theta}$ cannot be estimated perfectly.
However, Bayesian statistics can quantify
the uncertainty of the unknown parameters by treating them as random variables.
Treating $\bm{\theta}$ as a random variable also requires
specifying the \textit{prior} distribution $\pi(\bm{\theta})$,
which represents the uncertainty about the parameters before observing the data.
Bayes' theorem can then be applied
to obtain the \textit{posterior} distribution $p(\bm{\theta} | \bm{Y})$:
\begin{align}
	p(\bm{\theta} | \bm{Y})
		&= \frac{\mathcal{L}(\bm{Y} | \bm{\theta}) \pi(\bm{\theta})}{m(\bm{Y})}
		\propto \mathcal{L}(\bm{Y} | \bm{\theta}) \pi(\bm{\theta})
		\\
	m(\bm{Y})
		&= \int_{\Theta} \mathcal{L}(\bm{Y} | \bm{\theta'}
		\cdot \pi(\bm{\theta'}) d\theta'
\end{align}
where $m(\bm{Y})$ is the marginal likelihood or \textit{evidence} of $\bm{Y}$
over the entire parameter space $\Theta$.
It represents the probability of generating the observed sample
for all possible values of the parameters.
Due to the integration over the parameter space,
the evidence does not directly depend on parameters.
Often times, the evidence is simply the normalizing constant
that ensures the posterior is a proper probability distribution.
The posterior quantifies the uncertainty about the parameters that remains
after accounting for prior knowledge
and the new information observed in the data.

\subsection{Markov Chain Monte Carlo}

In real-world applications,
calculating the posterior distribution analytically can be intractable.
The main bottleneck is the computation of $m(\bm{Y})$,
which may require evaluating a high-dimensional integral
over the parameter space $\Theta$.
For complex models, this integral often lacks a closed-form solution.
Markov chain Monte Carlo (MCMC) algorithms bypass this problem
by sampling from the posterior distribution
without requiring the calculation of $m(\bm{Y})$.

MCMC combines two different concepts: Monte Carlo methods and Markov chains.
Monte Carlo methods rely on repeated random sampling to obtain results.
The underlying idea is to use randomness to solve problems
that might be deterministic in nature.
On the other hand, a Markov chain is a stochastic process describing
a sequence of events where the probability of an event depends only on
the state acquired in the previous event (memorylessness).
Given a probability distribution, one can design a Markov chain
whose stationary distribution matches the target distribution.
A Monte Carlo method can then be used
to generate the elements of that Markov chain.
With sufficient Monte Carlo samples,
the distribution of those elements approximates the posterior.

% TODO: cite
\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{sfigs/metropolis-hastings.png}
	\caption{
		Example of the Metropolis-Hastings algorithm in the Bayesian framework
		sampling a normal one-dimensional posterior distribution,
		where the prior is a uniform distribution.
	}
	\label{fig:mh}
\end{figure}

One of the most foundational MCMC algorithms is Metropolis-Hastings.
It starts with picking an initial parameter value $\theta^t = \theta^0$
at step $t = 0$.
At each iteration $t$, a candidate parameter $\theta^*$ is proposed
from a transition (or proposal) distribution $q(\theta^* | \theta^t)$.
The candidate is accepted with probability $\alpha$:
\begin{align}
	\alpha
	&= \min \left( 1,
		\frac{p(\bm{\theta}^* | \bm{Y}) q(\bm{\theta}^t | \bm{\theta}^*)}
			{p(\bm{\theta}^t | \bm{Y}) q(\bm{\theta}^* | \bm{\theta}^t)}
		\right)
\end{align}
Because the posterior appears in both the numerator and denominator of the ratio,
the evidence $m(\bm{Y})$ cancels out:
\begin{align}
	\frac{p(\bm{\theta}^* | \bm{Y})}{p(\bm{\theta}^t | \bm{Y})}
	&= \frac{\mathcal{L}(\bm{Y} | \bm{\theta}) \pi(\bm{\theta}) / m(\bm{Y})}
		{\mathcal{L}(\bm{Y} | \bm{\theta}^t) \pi(\bm{\theta}^t) / m(\bm{Y})} \\
	&= \frac{\mathcal{L}(\bm{Y} | \bm{\theta}) \pi(\bm{\theta})}
		{\mathcal{L}(\bm{Y} | \bm{\theta}^t) \pi(\bm{\theta}^t)}
\end{align}
This allows us to sample from the posterior
using only the likelihood and the prior.
For symmetrical proposal distributions,
$q(\bm{\theta}^t | \bm{\theta}^*) = q(\bm{\theta}^* | \bm{\theta}^t)$.
If an uninformative prior, such as a uniform distribution, is used,
the priors in the acceptance ratio cancel out as well since
$\pi(\bm{\theta}) = \pi(\bm{\theta^t})$.
An illustration of the Metropolis-Hastings algorithm
is shown in Figure \ref{fig:mh}.

After a sufficient number of iterations $t = B$,
known as the \textit{burn-in} period,
the samples {$\theta^{B+1}, \ldots, \theta^T$}
can be treated as draws from the posterior.
These samples are then used to estimate various statistics,
such as the posterior mean, variance, or credible intervals:
\begin{align}
	E[\bm{\theta} | \bm{Y}]
	\approx \frac{1}{T-B} \sum_{t=B+1}^{T} \bm{\theta}^t
\end{align}
where $T$ is the total number of iterations.
Other popular variants of MCMC include Gibbs Sampling,
which is useful when conditional distributions are known,
and Hamiltonian Monte Carlo, which uses gradient information
to explore the parameter space more efficiently.
