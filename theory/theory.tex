\chapter{Theoretical Background}
\label{chap:theory}

\section{Molecular Dynamics}

Molecular dynamics (MD) is a computer simulation technique
where the time evolution of a set of interacting atoms
is calculated using equations of motion.
The laws of classical mechanics, such as Newton's second law,
is followed in this technique.
For example, in a closed system consisting of $N$ atoms,
the force acting on an atom $i$ is
\begin{align}
	\bm{F}_i = m \bm{a}_i = m \frac{d^2 \bm{r}_i}{dt^2}
\end{align}

This force is a result of the interactions of atom $i$
with all the other atoms in the system.
If the force acting on an atom is known, its movement can be predicted
given the initial position and velocity \cite{ercolessi}.
We can derive these forces
by computing the gradients of the potential energy of the system
with respect to atomic displacements.
\begin{align}
	\bm{F}_i = - \nabla_{\bm{r}_i} V(\bm{r}_1,\ldots,\bm{r}_N)
\end{align}

The potential energy of the system actually depends on the interaction
between the subatomic particles in the atoms.
Thus, the true interatomic interactions are quantum mechanical in nature.
It is not known whether the interactions described by
the Schr\"odinger equation or Dirac equation for all subatomic particles
could be cast into an analytical functional form
to describe the interactions between atoms.
Thus, all descriptions of interatomic potentials are approximations.

In \textit{ab initio} MD (AIMD), interatomic forces are calculated on-the-fly
using quantum mechanicaly methods, typically density functional theory (DFT).
Forces are obtained from the electronic wavefunction
computed at each time step using DFT,
while assuming electrons adapt instantaneously compared to nuclei positions.
This is known as Born-Oppenheimer approximation.
Although calculations from first-principles are accurate,
they are computationally expensive and sometimes infeasible for certain tasks.

Finding a practical and efficient description of the potential energy,
albeit approximate, is the crux of MD.
The classical interatomic potentials try to approximate
the true nature of atomic interactions at the expense of accuracy,
the advantage being reduced computational cost \cite{marx}.
Interatomic potentials can be either parametric or non-parametric.
We will discuss the differences between them in the following subsection.

After defining the interactions among atoms through a potential,
new positions and velocities of atoms can be calculated using time integration.
Velocity-Verlet is the most commonly used integration scheme in MD
\cite{verlet, tuckerman}.
In this scheme, positions, velocitiies, and accelerations at time $t + \Delta t$
are obtained from the same quantities at time $t$ in the following way:
\begin{align}
	\bm{r}(t + \Delta t)
		&= \bm{r}(t) + \bm{v}(t)\Delta t + \frac{1}{2}\bm{a}(t)\Delta t^2 \\
	\bm{v}(t + \Delta t)
		&= \bm{v}(t) + \frac{1}{2} \bm{a}(t) \Delta t \\
	\bm{a}(t + \Delta t)
		&= - \frac{1}{m} \nabla V (\bm{r}(t + \Delta t)) \\
	\bm{v}(t + \Delta t)
		&= \bm{v}\left(t + \frac{1}{2}\Delta t\right)
		+ \frac{1}{2}\bm{a}(t + \Delta t)\Delta t
\end{align}

This allows the simulated systems to evolve in time,
which in turn provides insight into the behavior of complex atomic systems.

MD simulations are mainly done in the microcanonical ensemble.
However, in many situations,
the simulated system needs to have a constant temperature.
Among many methods introduced to control the temperature in a MD simulation,
the Nos\'e-Hoover thermostat is the most popular one \cite{hoover}.

\subsection{Interatomic Potential}

Interatomic potentials come in different forms
to accomodate different physical motivations.
Historically, most interatomic potentials could be described as ``parametric",
since they are developed with a fixed number of physical terms and parameters.
In non-parametric potentials,
the total number of terms and parameters are flexible.
This allows systematic optimization of potentials
using complex local neighbor descriptors
and separate mappings to predict different system properties.
Even though the non-parametric potentials can be more accurate,
the lack of physical meaning behind the parameters
can make justification of extrapolation and uncertainty quantification harder
\cite{shapeev2016mtp}.

The simplest parametric interatomic potential
would only consider pairwise interactions between atoms,
using pairwise potential function $\phi_{\alpha\beta}$
between atom types $\alpha$ and $\beta$.
These two-body (pairwise) potentials are poor for metallic systems.
The embedded-atom method (EAM) overcomes this problem
by adding an embedding energy term to the pairwise potential
\cite{daw1984eam, daw1993eam}.
In EAM, the potential energy $V_i$ of an atom $i$ is given by
\begin{align}
	V_i
	= \frac{1}{2} \sum_{j \ne i} \phi_{\alpha\beta} (r_{ij})
		+ F_{\alpha} \bigg( \sum_{j \ne i} \rho_{\beta} (r_{ij}) \bigg)
\end{align}
where $r_{ij}$ is the distance
between atom $i$ of type $\alpha$ and neighbors $j$ of type $\beta$,
$\rho_{\beta}$ is the contribution to the electro charge density
from atom $j$ at the location of atom $i$,
and $F_{\alpha}$ is an embedding function that represents
the energy required to place atom $i$ into the electron cloud.

The angular-dependent potential (ADP) formalism
is a generalization of the EAM type potential
to capture the anisotropy of bonds in certain materials
\cite{mishin2005adp, beelerUMoXe}.
The potential energy $V_i$ of an atom $i$ in ADP is given by
\begin{align}
	V_i
	&= \frac{1}{2} \sum_{j \ne i} \phi_{\alpha\beta} (r_{ij})
		+ F_{\alpha} \bigg( \sum_{j \ne i} \rho_{\beta} (r_{ij}) \bigg)
		+ \frac{1}{2} \sum_{s} (\mu_i^s)^2
		+ \frac{1}{2} \sum_{s, t} (\lambda_i^{st})^2
		- \frac{1}{6} \sum_i \nu_i^2 \\
	\mu_i^s
	&= \sum_{j \ne i} u_{\alpha\beta} (r_{ij}) r_{ij}^s \\
	\lambda_i^{st}
	&= \sum_{j \ne i} w_{\alpha\beta} (r_{ij}) r_{ij}^s r_{ij}^t \\
	\nu_i
	&= \sum_s \lambda_i^{ss}
\end{align}
where $i$, $j$, $\alpha$, $\beta$, $\phi_{\alpha\beta}$, and $F_{\alpha}$
have the same definitions as in the EAM formalism.
Additionally, $s$ and $t = 1, 2, 3$ and refer to the Cartesian coordinates.
The $\mu$ and $\lambda$ terms represent dipole and quadrupole distortions
of the local atomic environment.
This formalism extends the EAM by introducing angular forces
as dipole and quadrupole moments.

The non-parametric potentials are often referred to
as machine-learned interatomic potentials (MLIPs) \cite{shapeev2016mtp}.
While non-parametric models are especially suitable for the application of ML,
it is to be noted that parametric potentials can also be optimized
using machine learning \cite{mishin2021}.
The potential energy of a system in an MLIP is written as
\begin{align}
	V_{tot}
	= \sum_i V_i (\bm{q}_i)
\end{align}
where $\bm{q}_i$ is a mathematical representation
of the atomic environment surrounding the atom $i$,
commonly known as the descriptor.
$V_i$ is a machine-learning model that provides energy prediction
based on the descriptor.
An MLIP requires a robust descriptor
that maintains certain physical symmetries,
and a suitable machine learning framework for the descriptor
\cite{deringer2019}.

\section{Binary Collision Approximation}

While MD provides a rigorous description of $N$-body interactions,
it becomes computationally intractable when simulating
high-energy ion irradiation due to small time steps and large spatial domains.
In MD, the timestep size $\Delta t$ is typically on the order of femstoseconds,
which is small enough to resolve the fastest atomic vibrations.
For a high energy ion traversing a solid material,
the vast majority of the simulation time in MD will be spent on
calculating small-amplitude oscillations of the lattice atoms
that do not significantly contribute to the trajectory of the ion
or the primary radiation damage it causes \cite{drobny2023}.

The Binary Collision Approximation (BCA) addresses this
by approximating the many-body problem with a sequence of
discrete two-body interactions that are assumed to be independent.
The rationale behind this is that at high kinetic energies,
the interaction time between a projectile and a target nucleus
is much shorter than the period of lattice vibrations.
Therefore, the target nuclei can be treated as stationary
and the effect of surrounding atoms on the projectile can be neglected
during the brief duration of the collision \cite{robinson1974, ziegler2010srim}.

\begin{figure}[ht]
	\centering
	\begin{tikzpicture}[scale=2, >={Stealth[length=15pt]}]
		% lab frame
		\begin{scope}
			\coordinate (A) at (0, 0);
			\coordinate (B) at (4, 0);
			\coordinate (C) at (0, 0.5);
			\coordinate (D) at (4, 0.5);
			\coordinate (E) at (3.5, 2);
			\coordinate (F) at (3.5, -1);
			\coordinate (G) at ($(C)!0.5!(D)$);
			\coordinate (H) at ($(A)!0.5!(B)$);
			\coordinate (I) at ($(C)+(0.2,0)$);

			\node [anchor=west] at (0, 2) {\small (a) Lab frame};

			% horizontal lines
			\draw[dashed, thin] (A) -- (B);
			\draw[dashed, thin] (C) -- (D);

			% ion
			\shade[ball color=gray!80, opacity=0.8]
				(I) circle (0.15);
			\draw[yellow, line width=3pt, opacity=0.3]
				(I) circle (0.15);

			% target
			\shade[ball color=red!80, opacity=0.8]
				(H) circle (0.15);
			\draw[yellow, line width=5pt, opacity=0.3]
				(H) circle (0.15);

			% trajectories
			\draw[thick, ->]
				(I) .. controls (G) and (G) .. (E);
			\draw[thick, ->] (H) -- (F);

			% angle
			\draw[dashed] (G) -- (E);
			\draw ($(G)+(1,0)$) arc (0:45:1)
				node[midway, right, xshift=2pt] {$\theta$};
		\end{scope}

		% center of mass frame
		\begin{scope}[shift={(0,-3.5)}]
			\coordinate (A) at (0, 0);
			\coordinate (B) at (4, 0);
			\coordinate (C) at (0, 0.5);
			\coordinate (D) at (4, 0.5);
			\coordinate (I) at ($(C)+(0.6,0)$);
			\coordinate (T) at ($(B)-(0.6,0)$);
			\coordinate (K) at (1, -1.2);
			\coordinate (M) at (3, 2);
			\coordinate (P) at ($(K)+(0.7,0)$);
			\coordinate (R) at ($(M)+(0.7,0)$);

			\node [anchor=west] at (0, 2) {\small (b) Center of mass frame};

			% horizontal lines
			\draw[dashed, thin, name path=ab] (A) -- (B);
			\draw[dashed, thin, name path=cd] (C) -- (D);

			% oblique lines
			\draw[dashed, thin, name path=km] (K) -- (M);
			\draw[dashed, thin, name path=pr] (P) -- (R);

			\path[name intersections={of=km and cd, by=L}];
			\path[name intersections={of=ab and pr, by=Q}];

			% ion
			\shade[ball color=gray!80, opacity=0.8]
				(I) circle (0.15);
			\draw[yellow, line width=3pt, opacity=0.3]
				(I) circle (0.15);

			% target
			\shade[ball color=red!80, opacity=0.8]
				(T) circle (0.15);
			\draw[yellow, line width=5pt, opacity=0.3]
				(T) circle (0.15);

			% trajectories
			\draw[thick, ->]
				(I) .. controls (L) and (L) .. (M);
			\draw[thick, ->]
				(T) .. controls (Q) and (Q) .. (P);

			% angle
			\draw ($(L)+(0.6,0)$) arc (0:58:0.6)
				node[midway, right, xshift=2pt] {$\Theta$};

			% p and r_0
			\begin{scope}[>={Stealth[length=5pt]}]
				\draw[<->] ($(A)+(0.2,0)$) -- ($(C)+(0.2,0)$)
					node[midway, right] {$p$};
				\draw[<->] ($(L)+(-0.1,0.14)$) -- ($(Q)+(0.05,-0.10)$)
					node[midway, right] {$r_{min}$};
			\end{scope}
		\end{scope}
	\end{tikzpicture}
	\caption{
		Elastic collision between a projectile and a target in
		(a) the lab frame and (b) the center of mass frame.
	}
\end{figure}

The main event in BCA is the elastic collision
between a projectile of mass $M_1$ and energy $E_0$
and a stationary target of mass $M_2$.
After transforming the collision into the center of mass (CM) frame,
the laws of conservation of energy and momentum can be used to derive
the kinetic energy $T$ transferred to the target nucleus:
\begin{align}
	T = \frac{4 M_1 M_2}{(M_1 + M_2)^2} E_0 \sin^2\left(\frac{\Theta}{2}\right)
\end{align}
where $\Theta$ is the scattering angle in the CM frame.
This scattering angle is dependent on the impact parameter $p$,
which is the perpendicular distance between the initial
velocity vector of the projectile and the target \cite{ziegler2010srim}.

The angle $\Theta$ is calculated using the classical scattering integral:
\begin{align}
	\Theta = \pi - 2p \int_{r_{min}}^{\infty}
		\frac{dr}{r^2 \sqrt{1 - \frac{V(r)}{E_c} - \frac{p^2}{r^2}}}
\end{align}
where $E_c = E_0 M_2 / (M_1 + M_2)$ is the energy in the CM frame,
$V(r)$ is the interatomic potential,
and $r_{min}$ is the distance of closest approach.
The distance $r_{min}$ is the positive root of the denominator:
\begin{align}
	1 - \frac{V(r_{min})}{E_c} - \frac{p^2}{r_{min}^2} = 0
\end{align}

In the context of BCA,
the interatomic potential $V(r)$ must describe
the repulsive force at very short internuclear distances accurately.
Unlike the EAM or ADP potentials used in MD,
BCA usually employs screened Coulomb potentials
to account for the electrostatic repulsion between nuclei,
moderated by the electron clouds.
The general form of this interatomic potential is
\begin{align}
	V(r) = \frac{Z_1 Z_2 e^2}{4\pi\epsilon_0 r} \Phi\left(\frac{r}{a}\right)
\end{align}
where $Z_1$ and $Z_2$ are the atomic numbers, $e$ is the elementary charge,
and $\Phi(r/a)$ is a screening function with a screening length $a$.
The Universal Ziegler-Biersack-Littmark (ZBL) potential
is the most widely used screening function in BCA codes,
such as TRIM \cite{ziegler1985}.
Another popular screening function is based on the Kr-C interatomic potential
used in Tridyn \cite{moller1984}:
\begin{align}
	\Phi(x) &= 0.19 e^{-0.28 x} + 0.46 e^{-0.64 x} + 0.34 e^{-1.9 x}
\end{align}

Beside losing energy through elastic nuclear collisions,
ions also lose energy to the target electrons.
This electronic stopping power is often treated as
a continuous dissipative force, analogous to a friction term.
The total energy loss per unit path length is thus
the sum of nuclear and electronic losses:
\begin{align}
	\frac{dE}{dx} = \left( \frac{dE}{dx} \right)_{n} + \left( \frac{dE}{dx} \right)_{e}
\end{align}
Nuclear stopping $(dE/dx)_n$ arises from
the discrete transfer of momentum to target nuclei ($T$),
whereas electronic stopping $(dE/dx)_e$ accounts for
excitations and ionizations of the target's electrons.

In the BCA algorithm, the ion's energy is reduced by
$T$ and the accumulated electronic loss along the path length iteratively.
The ion's direction is updated based on the scattering angle $\theta$
in the laboratory frame:
\begin{align}
	\theta &= \arctan \left( \frac{M_2 \sin \Theta}{M_1 + M_2 \cos \Theta} \right)
\end{align}
This process continues until
the ion's energy falls below a specified cutoff energy,
signifying that the ion has come to rest within the target.

Compared to MD, the BCA allows the simulation of millions of ion impacts
in a fraction of the time,
making it the better choice for calculating
sputtering yields, damage production, ion implantation profiles, etc.
\cite{drobny2023, matthews2015diss}

\section{Bayesian Statistics}

The probability of two events $A$ and $B$ happening together
can be expressed as:
\begin{align}
	P(A \cap B) &= P(B) P(A|B) = P(A) P(B|A)
\end{align}
where $P(B)$ means the probability of $B$ occurring,
and $P(A|B)$ denotes the probability of
$A$ occurring given $B$ already occurred.
Put simply, $P(A|B)$ is called
the \textit{conditional} probability of $A$ given $B$.
$P(A)$ and $P(B|A)$ have similar meanings.
Rewriting the above equation leads to:
\begin{align}
	P(A|B) &= \frac{P(B|A)P(A)}{P(B)}
\end{align}
where $P(B) \ne 0$.
This is known as the Bayes' theorem \cite{downey2021}.

Bayesian statistics is a subfield of statistics that utilizes
a particular interpretation of probability based on the Bayes' theorem.
In this interpretation, probability signifies a degree of belief in an event.
The degree of belief may depend on prior knowledge about the event,
such as previous experimental results or personal beliefs.
Specifically, the Bayesian interpretation has a way of
incorporating prior knowledge in the form of probability distributions.
In contrast, the frequentist interpretation views probability as
the limit of the relative frequency of an event after many trials
\cite{gelman1995book}.

To expand on the Bayesian interpretation, let's assume
$\bm{Y} = (Y_1, \ldots, Y_n)$ is the data produced by a random process
and the random process can be modeled in terms of fixed but unknown parameters
$\bm{\theta} = (\theta_1, \ldots, \theta_p)$.
Then, the probability distribution $\mathcal{L}(\bm{Y} | \bm{\theta})$
indicates the \textit{likelihood} of $\bm{Y}$ given parameters $\bm{\theta}$.
Bayesian inference is concerned with the inverse problem of
estimating $\bm{\theta}$ using the likelihood function.
Since almost all data have noise, $\bm{\theta}$ cannot be estimated perfectly.
However, Bayesian statistics can quantify
the uncertainty of the unknown parameters by treating them as random variables.
Treating $\bm{\theta}$ as a random variable also requires
specifying the \textit{prior} distribution $\pi(\bm{\theta})$,
which represents the uncertainty about the parameters before observing the data.
Bayes' theorem can then be applied
to obtain the \textit{posterior} distribution $p(\bm{\theta} | \bm{Y})$:
\begin{align}
	p(\bm{\theta} | \bm{Y})
		&= \frac{\mathcal{L}(\bm{Y} | \bm{\theta}) \pi(\bm{\theta})}{m(\bm{Y})}
		\propto \mathcal{L}(\bm{Y} | \bm{\theta}) \pi(\bm{\theta})
		\\
	m(\bm{Y})
		&= \int_{\Theta} \mathcal{L}(\bm{Y} | \bm{\theta'})
		\pi(\bm{\theta'}) d\theta'
\end{align}
where $m(\bm{Y})$ is the marginal likelihood or \textit{evidence} of $\bm{Y}$
over the entire parameter space $\Theta$.
It represents the probability of generating the observed sample
for all possible values of the parameters.
Due to the integration over the parameter space,
the evidence does not directly depend on parameters.
Often times, the evidence is simply the normalizing constant
that ensures the posterior is a proper probability distribution.
The posterior quantifies the uncertainty about the parameters that remains
after accounting for prior knowledge
and the new information observed in the data \cite{gelman1995book, reich2019}.

\subsection{Markov Chain Monte Carlo}

In real-world applications,
calculating the posterior distribution analytically can be intractable.
The main bottleneck is the computation of $m(\bm{Y})$,
which may require evaluating a high-dimensional integral
over the parameter space $\Theta$.
For complex models, this integral often lacks a closed-form solution.
Markov chain Monte Carlo (MCMC) algorithms bypass this problem
by sampling from the posterior distribution
without requiring the calculation of $m(\bm{Y})$ \cite{reich2019}.

MCMC combines two different concepts: Monte Carlo methods and Markov chains.
Monte Carlo methods rely on repeated random sampling to obtain results.
The underlying idea is to use randomness to solve problems
that might be deterministic in nature.
On the other hand, a Markov chain is a stochastic process describing
a sequence of events where the probability of an event depends only on
the state acquired in the previous event (memorylessness).
Given a probability distribution, one can design a Markov chain
whose stationary distribution matches the target distribution.
A Monte Carlo method can then be used
to generate the elements of that Markov chain.
With sufficient Monte Carlo samples,
the distribution of those elements approximates the posterior \cite{geyer1992}.

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{sfigs/metropolis-hastings.png}
	\caption{
		Example of the Metropolis-Hastings algorithm in the Bayesian framework
		sampling a normal one-dimensional posterior distribution,
		where the prior is a uniform distribution.
		\textit{Reproduced from Lee et al. \cite{lee2015}.}
	}
	\label{fig:mh}
\end{figure}

One of the most foundational MCMC algorithms is Metropolis-Hastings.
It starts with picking an initial parameter value
$\bm{\theta}^t = \bm{\theta}^0$ at step $t = 0$.
At each iteration $t$, a candidate parameter $\bm{\theta}^*$ is proposed
from a transition (or proposal) distribution $q(\bm{\theta}^* | \bm{\theta}^t)$.
The candidate is accepted with probability $\alpha$:
\begin{align}
	\alpha
	&= \min \left( 1,
		\frac{p(\bm{\theta}^* | \bm{Y}) q(\bm{\theta}^t | \bm{\theta}^*)}
			{p(\bm{\theta}^t | \bm{Y}) q(\bm{\theta}^* | \bm{\theta}^t)}
		\right)
\end{align}
Because the posterior appears in both the numerator and denominator of the ratio,
the evidence $m(\bm{Y})$ cancels out:
\begin{align}
	\frac{p(\bm{\theta}^* | \bm{Y})}{p(\bm{\theta}^t | \bm{Y})}
	&= \frac{\mathcal{L}(\bm{Y} | \bm{\theta}^*) \pi(\bm{\theta}^*) / m(\bm{Y})}
		{\mathcal{L}(\bm{Y} | \bm{\theta}^t) \pi(\bm{\theta}^t) / m(\bm{Y})} \\
	&= \frac{\mathcal{L}(\bm{Y} | \bm{\theta}^*) \pi(\bm{\theta}^*)}
		{\mathcal{L}(\bm{Y} | \bm{\theta}^t) \pi(\bm{\theta}^t)}
\end{align}
This allows us to sample from the posterior
using only the likelihood and the prior \cite{gelman1995book}.
For symmetrical proposal distributions,
$q(\bm{\theta}^t | \bm{\theta}^*) = q(\bm{\theta}^* | \bm{\theta}^t)$.
If an uninformative prior, such as a uniform distribution, is used,
the priors in the acceptance ratio cancel out as well since
$\pi(\bm{\theta}^*) = \pi(\bm{\theta^t})$.
An illustration of the Metropolis-Hastings algorithm
is shown in Figure \ref{fig:mh}.

After a sufficient number of iterations $t = B$,
known as the \textit{burn-in} period,
the samples {$\bm{\theta}^{B+1}, \ldots, \bm{\theta}^T$}
can be treated as draws from the posterior.
These samples are then used to estimate various statistics,
such as the posterior mean, variance, or credible intervals:
\begin{align}
	E[\bm{\theta} | \bm{Y}]
		\approx \frac{1}{T-B} \sum_{t=B+1}^{T} \bm{\theta}^t
\end{align}
where $T$ is the total number of iterations.
Other popular variants of MCMC include Gibbs Sampling,
which is useful when conditional distributions are known,
and Hamiltonian Monte Carlo, which uses gradient information
to explore the parameter space more efficiently \cite{reich2019}.
