\chapter{Theoretical Background}

\section{Molecular Dynamics}

Molecular dynamics (MD) is a computer simulation technique
where the time evolution of a set of interacting atoms
is calculated using equations of motion.
The laws of classical mechanics, such as Newton's second law,
is followed in this technique.
For example, in a closed system consisting of $N$ atoms,
the force acting on an atom $i$ is
\begin{align}
	\bm{F}_i = m \bm{a}_i = m \frac{d^2 \bm{r}_i}{dt^2}
\end{align}

This force is a result of the interactions of atom $i$
with all the other atoms in the system.
If the force acting on an atom is known, its movement can be predicted
given the initial position and velocity \cite{ercolessi}.
We can derive these forces
by computing the gradients of the potential energy of the system
with respect to atomic displacements.
\begin{align}
	\bm{F}_i = - \nabla_{\bm{r}_i} V(\bm{r}_1,\ldots,\bm{r}_N)
\end{align}

The potential energy of the system actually depends on the interaction
between the subatomic particles in the atoms.
Thus, the true interatomic interactions are quantum mechanical in nature.
It is not known whether the interactions described by
the Schr\"odinger equation or Dirac equation for all subatomic particles
could be cast into an analytical functional form
to describe the interactions between atoms.
Thus, all descriptions of interatomic potentials are approximations.

In \textit{ab initio} MD (AIMD), interatomic forces are calculated on-the-fly
using quantum mechanicaly methods, typically density functional theory (DFT).
Forces are obtained from the electronic wavefunction
computed at each time step using DFT,
while assuming electrons adapt instantaneously compared to nuclei positions.
This is known as Born-Oppenheimer approximation.
Although calculations from first-principles are accurate,
they are computationally expensive and sometimes infeasible for certain tasks.

Finding a practical and efficient description of the potential energy,
albeit approximate, is the crux of MD.
The classical interatomic potentials try to approximate
the true nature of atomic interactions at the expense of accuracy,
the advantage being reduced computational cost \cite{marx}.
Interatomic potentials can be either parametric or non-parametric.
We will discuss the differences between them in the following section.

After defining the interactions among atoms through a potential,
new positions and velocities of atoms can be calculated using time integration.
Velocity-Verlet is the most commonly used integration scheme in MD
\cite{verlet, tuckerman}.
This allows the simulated systems to evolve in time,
which in turn provides insight into the behavior of complex atomic systems.

MD simulations are mainly done in the microcanonical ensemble.
However, in many situations,
the simulated system needs to have a constant temperature.
Among many methods introduced to control the temperature in a MD simulation,
the Nos\'e-Hoover thermostat is the most popular one \cite{hoover}.

\section{Interatomic Potential}

% TODO: citation needed
Interatomic potentials come in different forms
to accomodate different physical motivations.
Historically, most interatomic potentials could be described as "parametric",
since they are developed with a fixed number of physical terms and parameters.
In non-parametric potentials,
the total number of terms and parameters are flexible.
This allows systematic optimization of potentials
using complex local neighbor descriptors
and separate mappings to predict different system properties.
Even though the non-parametric potentials can be more accurate,
the lack of physical meaning behind the parameters
can make justification of extrapolation and uncertainty quantification harder.

The simplest parametric interatomic potential
would only consider pairwise interactions between atoms,
using pairwise potential function $\phi_{\alpha\beta}$
between atom types $\alpha$ and $\beta$.
These two-body (pairwise) potentials are poor for metallic systems.
The embedded-atom method (EAM) overcomes this problem
by adding an embedding energy term to the pairwise potential
\cite{daw1984eam, daw1993eam}.
In EAM, the potential energy $V_i$ of an atom $i$ is given by
\begin{align}
	V_i
	= \frac{1}{2} \sum_{j \ne i} \phi_{\alpha\beta} (r_{ij})
		+ F_{\alpha} \bigg( \sum_{j \ne i} \rho_{\beta} (r_{ij}) \bigg)
\end{align}
where $r_{ij}$ is the distance
between atom $i$ of type $\alpha$ and neighbors $j$ of type $\beta$,
$\rho_{\beta}$ is the contribution to the electro charge density
from atom $j$ at the location of atom $i$,
and $F_{\alpha}$ is an embedding function that represents
the energy required to place atom $i$ into the electron cloud.

% TODO: citation needed
The non-parametric potentials are often referred to
as machine-learned interatomic potentials (MLIPs).
While non-parametric models are especially suitable for the application of ML,
it is to be noted that
parametric potentials can also be optimized using machine learning,
The potential energy of a system in an MLIP is written as
\begin{align}
	V_{tot}
	= \sum_i V_i (\bm{q}_i)
\end{align}
where $\bm{q}_i$ is a mathematical representation
of the atomic environment surrounding the atom $i$,
commonly known as the descriptor.
$V_i$ is a machine-learning model that provides energy prediction
based on the descriptor.
An MLIP requires a robust descriptor
that maintains certain physical symmetries,
and a suitable machine learning framework for the descriptor.

\subsection{Angular Dependent Potential}

% TODO: why ADP?

The angular dependent potential (ADP) formalism
is a generalization of the EAM type potential.
The potential energy $V_i$ of an atom $i$ in ADP is given by
\begin{align}
	V_i
	&= \frac{1}{2} \sum_{j \ne i} \phi_{\alpha\beta} (r_{ij})
		+ F_{\alpha} \bigg( \sum_{j \ne i} \rho_{\beta} (r_{ij}) \bigg)
		+ \frac{1}{2} \sum_{s} (\mu_i^s)^2
		+ \frac{1}{2} \sum_{s, t} (\lambda_i^{st})^2
		- \frac{1}{6} \sum_i \nu_i^2 \\
	\mu_i^s
	&= \sum_{j \ne i} u_{\alpha\beta} (r_{ij}) r_{ij}^s \\
	\lambda_i^{st}
	&= \sum_{j \ne i} w_{\alpha\beta} (r_{ij}) r_{ij}^s r_{ij}^t \\
	\nu_i
	&= \sum_s \lambda_i^{ss}
\end{align}
where $i$, $j$, $\alpha$, $\beta$, $\phi_{\alpha\beta}$, and $F_{\alpha}$
have the same definitions as in the EAM formalism.
Additionally, $s$ and $t = 1, 2, 3$ and refer to the Cartesian coordinates.
The $\mu$ and $\lambda$ terms represent dipole and quadrupole distortions
of the local atomic environment.
This formalism extends the EAM by introducing angular forces
as dipole and quadrupole moments.

% TODO: cite Mishin and Beeler

\subsection{Moment Tensor Potential}

Moment tensor potential (MTP) is a polynomial-like MLIP
based on the tensors of inertia of atomistic environments.
MTP is a local potential, i.e., the energy $E^{MTP}$ of a system
is the sum of contributions $V^{MTP} (n_i)$ of atomic neighborhoods $n_i$
for $N$ atoms.
\begin{align}
	E^{mtp} (cfg) = \sum_{i=1}^N V (n_i)
\end{align}
Each neighborhood is a tuple
$n_i = (\{r_{i1}, z_i, z_1\}, \ldots, \{r_{ij}, z_i, z_j\}, \ldots,
\{r_{iN_{nbh}}, z_i, z_{N_{nbh}}\})$,
where $r_{ij}$ are relative atomic positions,
$z_i$, $z_j$ are the element types of central and neighboring atoms,
and $N_{nbh}$ is the number of atoms in the neighborhood.

Each contribution $V (n_i)$ in the potential energy $E^{mtp}$
expands through a set of basis functions.
\begin{align}
	V (n_i)
	= \sum_{\alpha} \xi_{\alpha} B_{\alpha} (n_i)
\end{align}
where $B_{\alpha}$ are the MTP basis functions,
$\xi = \{\xi_{\alpha}\}$ are the parameters to be found
by fitting them to the training set.

To define the functional form of the basis functions $B_{\alpha}$,
moment tensor descriptors, or simply \textit{moments}, are introduced:
\begin{align}
	M_{\mu, \nu} (n_i)
	= \sum_j f_{\mu} (|r_{ij}|, z_i, z_j) r_{ij}^{\otimes \nu}
\end{align}
These descriptors consist of the radial and the angular part.
The radial part has the following form.
\begin{align}
	f_{\mu} (|r_{ij}, z_i, z_j)
	= \sum_{\beta=1}^Q c_{\mu, z_i, z_j}^{(\beta)} Q^{(\beta)} (|r_{ij}|)
\end{align}
where $\bm{c} = \{ c_{\mu, z_i, z_j}^{(\beta)} \}$
is the set of radial parameters,
and $Q^{\beta} (|r_{ij}|)$ are the radial basis functions.
\begin{align}
	Q^{(\beta)} (|r_{ij}|) =
	\begin{cases}
		\varphi^{(\beta)} (|r_{ij}|) (R_{cut} - |r_{ij}|)^2
			& |r_{ij}| < R_{cut} \\
		0
			& |r_{ij}| \geq R_{cut}
	\end{cases}
\end{align}
Here $\varphi^{(\beta)}$ are polynomial functions (e.g. Chebyshev polynomials)
on the interval $[R_{min}, R_{cut}]$,
where $R_{min}$ is the minimal distance between atoms in the system,
$R_{cut}$ is the cutoff radius which ensures smooth behavior of the potential
when atoms leave or enter the interaction neighborhood.


% TODO: cite Shapeev and Novikov

\section{Inverse Uncertainty Quantification}

Inverse uncertainty quantification (IUQ) is a process
to quantify uncertainties in the input parameters of a computer model
given experimental data.
IUQ is an essential step in computational model validation
because it provides a concrete and quantifiable measure of uncertainty
in model predictions
\cite{wu2018inverse, nagel2019bayesian, wu2021comprehensive}.
The concept of IUQ is in contrast to forward uncertainty quantification (FUQ),
which quantifies the uncertainty in the output of a computer model
given the input parameters.
FUQ is often used to predict the uncertainty in the results of a simulation,
while IUQ is used to quantify the uncertainty in the model itself.
IUQ is often used in engineering and scientific applications
where there is a high degree of uncertainty in the input parameters.
By quantifying the uncertainty in the input parameters,
IUQ can help to improve the accuracy and reliability
of the model predictions \cite{wu2021comprehensive, xie2024functional}.

\subsection{Bayesian Framework for IUQ}

Let's define the unknown reality or true value of an output as $y^R(x)$,
where $x$ denotes the vector of design variables
specifying experimental conditions.
A computer model simulation can predict that reality only as an approximation:
\begin{align}
	\label{eq:real}
	\bm{y}^R(\bm{x}) = \bm{y}^M(\bm{x}, \bm{\theta}^*) + \delta(\bm{x})
\end{align}
where $\bm{\theta}^*$ is a vector of true but unknown values
of calibration parameters $\bm{\theta}$
and $\delta(\bm{x})$ is the model uncertainty/discrepancy
due to an incomplete understanding of the underlying physics of the model.

To learn the reality $\bm{y}^R(\bm{x})$,
experiments may be performed to obtain an observation $\bm{y}^E(\bm{x})$.
However, the measurement process also introduces uncertainty:
\begin{align}
	\label{eq:exp}
	\bm{y}^E(\bm{x}) = \bm{y}^R(\bm{x}) + \bm{\epsilon}
\end{align}
where $\bm{\epsilon} \sim \mathcal{N} (\bm{\mu}, \bm{\Sigma}_{exp})$
indicates the measurement error/noise.
It is typical to assume $\bm{\mu} = 0$
and $\bm{\Sigma}_{exp} = \sigma_{exp}^2 \bm{I}$ for experiments
having no systematic bias and having homoscedastic experimental errors.
Combining equations \ref{eq:real} and \ref{eq:exp},
we can obtain the "model updating equation"
\cite{wu2021comprehensive, kennedy2001bayesian, arendt2012quant}:
\begin{align}
	\label{eq:upd}
	\bm{y}^E(\bm{x})
	= \bm{y}^M(\bm{x}, \bm{\theta}^*) + \delta(\bm{x}) + \bm{\epsilon}
\end{align}
Equation \ref{eq:upd} is the starting point for Bayesian IUQ.

Assuming experimental uncertainty is Gaussian,
$\bm{\epsilon}
= \bm{y}^E(\bm{x}) - \bm{y}^M(\bm{x}, \bm{\theta}^*) - \delta(\bm{x})$
follows a multi-dimensional Gaussian distribution
with a mean of zero and a covariance matrix of $\bm{\Sigma}$.
The posteriors of the true parameters $p(\bm{\theta}^* | \bm{y}^E, \bm{y}^M)$
can then be written as:
\begin{align}
	p(\bm{\theta}^* | \bm{y}^E, \bm{y}^M)
	&\propto p(\bm{\theta}^*) \cdot p(\bm{y}^E, \bm{y}^M | \bm{\theta}^*) \\
	&\propto p(\bm{\theta}^*) \cdot \frac{1}{\sqrt{|\bm{\Sigma}|}}
		\exp \bigg[-\frac{1}{2} (\bm{y}^E-\bm{y}^M-\delta)^T
		\bm{\Sigma}^{-1} (\bm{y}^E-\bm{y}^M-\delta) \bigg]
\end{align}
where $p(\bm{\theta}^*)$ is the prior distribution provided by expert opinion,
and $p(\bm{y}^E, \bm{y}^M | \bm{\theta}^*)$ is the likelihood function.
$\bm{\Sigma}$ is the covariance of the likelihood consisting of three parts:
\begin{align}
	\bm{\Sigma} &= \bm{\Sigma}_{exp} + \bm{\Sigma}_{bias} + \bm{\Sigma}_{code}
\end{align}
where $\bm{\Sigma}_{exp}$ represents experimental uncertainty
due to measurement error,
$\bm{\Sigma}_{bias}$ means model uncertainty
due to an inherent bias in the model,
and $\bm{\Sigma}_{code}$ means code/interpolation uncertainty
due to the use of surrogate models to reduce the computational cost.
