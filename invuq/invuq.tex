\chapter{Inverse Uncertainty Quantification
of \texorpdfstring{$\gamma$}{gamma}U-10Mo Fission-Gas-Behavior Parameters}

\section{Problem Definition}

% TODO: make the variable names monospaced

Table \ref{tab:uqparam} shows the DART parameters of interest and their ranges.
The orders of magnitude of the parameters vary considerably.
The parameter "dGrainHBS" denotes the high burunup structure grain diameter.
"FaceCovMax" dictates the grain face coverage needed for interlinkage.
"SwellLink" determines the swelling starting interlinkage of the grain edges.
"vResol" is the probability that a gas bubble interacts with fission fragments.
"DatomFissGBx" is the grain boundary diffusion enhancement factor.
"fNucleate" indicates the adjustment factor
for the probability of bubble nucleation on the grain boundary.
Finally, 'aAtomDifFiss" is the linear coefficient
of radiation-driven gas atom diffusivity
\cite{annualreport2021, ye2023, annualreport2022}.

\begin{table}[ht]
\centering
\caption{
	Fission gas behavior parameters and their ranges.
}
\label{tab:uqparam}
\begin{tabular}{lccc}
\toprule
Parameter    & Minimum value & Maximum value & Reference value \\
\midrule
dGrainHBS    & \num{2.4e-5 } & \num{5.6e-5 } & \num{4e-5   }   \\
FaceCovMax   & \num{0.6    } & \num{0.907  } & \num{0.907  }   \\
SwellLink    & \num{0.016  } & \num{0.034  } & \num{0.025  }   \\
vResol       & \num{1.3e-18} & \num{2.7e-18} & \num{2e-18  }   \\
DatomFissGBx & \num{19641  } & \num{40530  } & \num{3e4    }   \\
fNucleate    & \num{3.5e-10} & \num{8.3e-10} & \num{6e-10  }   \\
aAtomDifFiss & \num{3.2e-31} & \num{7.2e-31} & \num{5.1e-31}   \\
\bottomrule
\end{tabular}
\end{table}

The goal of this IUQ study is to find parameter distributions
that lead to fuel swelling predictions
in agreement with the experimental observations.
There have been a few experimental studies
on the fuel swelling of $\gamma$U-Mo fuel.
Robinson et al. have developed a predictive swelling correlation
using the experimental data collected on
74 irradiated $\gamma$U-10Mo monolithic test fuel plates
over a range of irradiation conditions
\cite{rabin2017preliminary, robinson2021}.
The correlation is as follows.
\begin{align}
	\% Swelling = 6.13 \times 10^{-43} F_d^2 + 4 \times 10^{-21} F_d
\end{align}
where $F_d$ is the fission density.
The 95\% confidence interval of the swelling prediction
at $7 \times 10^{21}$ fiss/cm$^3$ spans about 5 swelling\%.
This swelling correlation is utilized as the experimental observation
for IUQ purposes in this work.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.6\textwidth]{invuq/images/target.png}
	\caption{
		Histogram of the fuel swelling prediction from DART
		at $7 \times 10^{21}$ fiss/cm$^3$
		along with the experimental observation.
	}
	\label{fig:target}
\end{figure}

\section{Methodology}

Inverse uncertainty quantification (IUQ) is a process
to quantify uncertainties in the input parameters of a computer model
given experimental data.
IUQ is an essential step in computational model validation
because it provides a concrete and quantifiable measure of uncertainty
in model predictions
\cite{wu2018inverse, nagel2019bayesian, wu2021comprehensive}.
The concept of IUQ is in contrast to forward uncertainty quantification (FUQ),
which quantifies the uncertainty in the output of a computer model
given the input parameters.
FUQ is often used to predict the uncertainty in the results of a simulation,
while IUQ is used to quantify the uncertainty in the model itself.
IUQ is often used in engineering and scientific applications
where there is a high degree of uncertainty in the input parameters.
By quantifying the uncertainty in the input parameters,
IUQ can help to improve the accuracy and reliability
of the model predictions \cite{wu2021comprehensive, xie2024functional}.

\subsection{Bayesian Framework for IUQ}

Let's define the unknown reality or true value of an output as $y^R(x)$,
where $x$ denotes the vector of design variables
specifying experimental conditions.
A computer model simulation can predict that reality only as an approximation:
\begin{align}
	\label{eq:real}
	\bm{y}^R(\bm{x}) = \bm{y}^M(\bm{x}, \bm{\theta}^*) + \delta(\bm{x})
\end{align}
where $\bm{\theta}^*$ is a vector of true but unknown values
of calibration parameters $\bm{\theta}$
and $\delta(\bm{x})$ is the model uncertainty/discrepancy
due to an incomplete understanding of the underlying physics of the model.

To learn the reality $\bm{y}^R(\bm{x})$,
experiments may be performed to obtain an observation $\bm{y}^E(\bm{x})$.
However, the measurement process also introduces uncertainty:
\begin{align}
	\label{eq:exp}
	\bm{y}^E(\bm{x}) = \bm{y}^R(\bm{x}) + \bm{\epsilon}
\end{align}
where $\bm{\epsilon} \sim \mathcal{N} (\bm{\mu}, \bm{\Sigma}_{exp})$
indicates the measurement error/noise.
It is typical to assume $\bm{\mu} = 0$
and $\bm{\Sigma}_{exp} = \sigma_{exp}^2 \bm{I}$ for experiments
having no systematic bias and having homoscedastic experimental errors.
Combining equations \ref{eq:real} and \ref{eq:exp},
we can obtain the "model updating equation"
\cite{wu2021comprehensive, kennedy2001bayesian, arendt2012quant}:
\begin{align}
	\label{eq:upd}
	\bm{y}^E(\bm{x})
	= \bm{y}^M(\bm{x}, \bm{\theta}^*) + \delta(\bm{x}) + \bm{\epsilon}
\end{align}
Equation \ref{eq:upd} is the starting point for Bayesian IUQ.

Assuming experimental uncertainty is Gaussian,
$\bm{\epsilon}
= \bm{y}^E(\bm{x}) - \bm{y}^M(\bm{x}, \bm{\theta}^*) - \delta(\bm{x})$
follows a multi-dimensional Gaussian distribution
with a mean of zero and a covariance matrix of $\bm{\Sigma}$.
The posteriors of the true parameters $p(\bm{\theta}^* | \bm{y}^E, \bm{y}^M)$
can then be written as:
\begin{align}
	p(\bm{\theta}^* | \bm{y}^E, \bm{y}^M)
	&\propto p(\bm{\theta}^*) \cdot p(\bm{y}^E, \bm{y}^M | \bm{\theta}^*) \\
	&\propto p(\bm{\theta}^*) \cdot \frac{1}{\sqrt{|\bm{\Sigma}|}}
		\exp \bigg[-\frac{1}{2} (\bm{y}^E-\bm{y}^M-\delta)^T
		\bm{\Sigma}^{-1} (\bm{y}^E-\bm{y}^M-\delta) \bigg]
\end{align}
where $p(\bm{\theta}^*)$ is the prior distribution provided by expert opinion,
and $p(\bm{y}^E, \bm{y}^M | \bm{\theta}^*)$ is the likelihood function.
$\bm{\Sigma}$ is the covariance of the likelihood consisting of three parts:
\begin{align}
	\bm{\Sigma} &= \bm{\Sigma}_{exp} + \bm{\Sigma}_{bias} + \bm{\Sigma}_{code}
\end{align}
where $\bm{\Sigma}_{exp}$ represents experimental uncertainty
due to measurement error,
$\bm{\Sigma}_{bias}$ means model uncertainty
due to an inherent bias in the model,
and $\bm{\Sigma}_{code}$ means code/interpolation uncertainty
due to the use of surrogate models to reduce the computational cost.

In this work, we assumed $\bm{\Sigma}_{bias} = 0$ and $\bm{\Sigma}_{code} = 0$
because of the lack of experimental observations
and high fidelity of the surrogate models.

\subsection{Surrogate Modeling}

Surrogate models, also called metamodels, response surfaces or emulators,
are approximations of the input/output relation of the original computer model.
They are built from a limited number of full model runs (training data)
and a learning algorithm.
Metamodels usually take much less computational time than the full model
while maintaining the input/output relation to a desirable accuracy.
Once validated, metamodels can be used in uncertainty, sensitivity,
validation and optimization studies,
for which the original computer model can incur
an excessive computational burden
as hundreds or thousands of computer model simulations are needed
\cite{wu2018inverse}.

Any machine learning model that can learn from the training data
and predict an output relatively quickly
compared to the original computer model can be used as a surrogate model.
Typical examples of surrogate models include Moving Least-Squares (MLS),
Radial Basis Functions (RBF), Neural Networks (NN),
Support Vector Machines (SVM), Gaussian Processes (GP),
Polynomial Chaos Expansion (PCE), etc.,
with GP being the most commonly used surrogate
\cite{wu2018inverse, wu2021comprehensive}.
In this work, we have used three linear
and three non-linear models as surrogates.
A brief discussion of these models follows.

Considering $X$ as an input matrix and $y$ as an output vector,
a linear model would have the following formulation:
\begin{align}
	y = \beta X + \varepsilon
\end{align}
where $\beta$ is the vector of unknown coefficients
and $\varepsilon$ is the vector of random errors.
In Ordinary Least-Squares (OLS),
the estimated coefficients $\hat{\beta}$ is determined
by minimizing the sum of squared residuals (the loss function).
\begin{align}
	\hat{\beta} = \arg \min_{\beta} || y - \beta X ||^2
\end{align}
In Least Absolute Shrinkage and Selection Operator (Lasso),
$\hat{\beta}$ is determined
by adding an $L1$ penalty term to the loss function.
\begin{align}
	\hat{\beta} = \arg \min_{\beta} (|| y - \beta X ||^2
					+ \lambda \sum |\beta_i|)
\end{align}
where $\lambda$ is the regularization parameter
that adds a penalty for the magnitude of the coefficients $\beta$.
The benefit in using Lasso is that the added penalty
shrinks some coefficients to zero.
As a result, this regression model also acts as a selection operator.
Ridge regression is different from Lasso
because it uses an $L2$ regularization term to the loss function
\cite{yang2018ridge, thevaraja2019recent}.
The estimated coefficients then become:
\begin{align}
	\hat{\beta} = \arg \min_{\beta} (|| y - \beta X ||^2
					+ \lambda ||\beta||^2)
\end{align}

Gaussian processes (GP) are a popular choice for surrogate models
because they provide a straightforward estimation of the code uncertainty.
A GP is a probabilistic model
that describes a distribution over possible functions.
Given a set of points, a GP can be used to predict the value
of an unknown function at any other point.
Additionally, GP can be used to represent the uncertainty
in the output of the computer model,
which is important for Bayesian inference and calibration
\cite{wu2018inverse, wang2020}.
The mathematical form of a GP model is as follows:
\begin{align}
	y = f^T(x) \beta + z(x)
\end{align}
The first term here is a linear regression of the data
modeling the drift of the mean.
The set of basis functions $f$ is chosen by the user
and $\beta$ are the regression coefficients.
$z(x)$ is a stationary Gaussian random process with zero mean and covariance
$Cov[z(x^{(i)}), z(x^{(j)})] = \sigma^2 \mathcal{R} (x^{(i)}, x^{(j)})$,
where $\sigma^2$ is the process variance
and $\mathcal{R} (\cdot, \cdot)$ is the correlation function or kernel.
A kernel in GP is a function that defines
the similarity between pairs of data points.
There are many different kernels that can be used with GP,
each of which has its own advantages and disadvantages.
Some of the most commonly used kernel functions include the RBF kernel,
the Mat\'ern kernel, and the rational quadratic kernel \cite{wang2020}.

NN models consist of one input layer,
hidden layers with non-linear activation functions,
and one output layer for regression.
All these layers have nodes (also called neurons)
that connect to the nodes of the previous layer.
Each connection between two nodes has an optimizable weight
and each node has an associated bias and an activation function.
The number of hidden layers in the model,
the number of nodes in each layer,
and the activation functions of the nodes are to be tuned
to find an optimal model for a particular dataset
\cite{goodfellow2016, de2013neural}.

Support Vector Regressor (SVR) is a regression model
that uses SVMs, aiming to find a function $f(X)$
that fits the data while keeping the errors within a predefined margin.
\begin{align}
	f(X) = w^T X + b
\end{align}
where $w$ is the weight vector and $b$ is the bias.
SVR uses an $\varepsilon$-insensitive loss function,
where the model allows errors within a certain margin $\varepsilon$.
SVRs can handle non-linear relationships through the use of kernels,
which map input data to a higher-dimensional space
\cite{cortes1995support, gunn1997support}.
In this work, all the surrogate models are implemented
using the sklearn python library \cite{sklearn}.

\section{Results}

\subsection{Sensitivity Analysis}

There are 3200 samples relating the 7 fission-gas-behavior parameters
with fuel swelling at a fission density of $7 \times 10^{21}$ fiss/cm$^3$.
Figure \ref{fig:scatter} shows scatter plots of fuel swelling
against all the fission-gas-behavior parameters.
Except for the parameters "dGrainHBS" and "FaceCovMax",
all other parameters have weak correlation with fuel swelling.
To also understand the correlation among the parameters,
a correlation heatmap is depicted in Figure \ref{fig:heatmap}.
It is apparent that there is negligible correlation among the parameters.
This is expected since the compiled dataset was created
using fission-gas-behavior parameters sampled independently of each other.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{invuq/images/scatter.png}
	\caption{
		Scatter plots of fuel swelling
		against the fission-gas-behavior parameters.
	}
	\label{fig:scatter}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.6\textwidth]{invuq/images/corrheat.png}
	\caption{
		Correlation heatmap of
		fission-gas-behavior parameters and fuel swelling.
	}
	\label{fig:heatmap}
\end{figure}

\subsection{Surrogate Modeling}

The training and testing of the surrogate models
are done on a 70/30 split of the DART dataset.
Figure \ref{fig:linear} depicts the surrogate prediction vs actual test data
for three linear surrogate models: OLS, Lasso, and Ridge.
The performance of the linear surrogate models are assessed
with the test data.
The R$^2$ scores for all of them are at least 0.997,
and the root mean squared errors (RMSEs)
and the mean absolute errors (MAEs)
are about 0.25 and 0.17 swelling\%, respectively.
Table \ref{tab:surrogates} details a few performance metrics
for all the surrogate models.
Among the three linear surrogate models,
the Lasso model provided the most noteworthy insight.
The regression coefficients for the parameters
"vResol", "DatomFissGBx", "fNucleate", and "aAtomDifFiss"
are equal to 0 in the Lasso surrogate.
Therefore, it is possible to ignore these four parameters
without losing any accuracy in the prediction.

\begin{figure}[!ht]
\begin{subfigure}{0.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{invuq/images/ols.png}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{invuq/images/lasso.png}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{invuq/images/ridge.png}
\end{subfigure}
\caption{
	Surrogate prediction vs actual test data for linear surrogates.
}
\label{fig:linear}
\end{figure}

The prediction vs true test data for three non-linear surrogate models:
GP, NN, and SVR, are shown in Figure \ref{fig:nonlin}.
The NN and SVR are trained with 7 parameters
and 70\% of the total data (the train split),
similar to the linear models.
The GP model is trained with only 3 significant parameters
(as identified by Lasso) and 14\% of the total data (448 samples).
However, this model is evaluated with the same test data as others.
The rationale behind using fewer training samples for the GP model
is to reduce the computational cost.
It is apparent from Figure \ref{fig:nonlin}
that the GP model predict swelling values with high accuracy.
This is also reflected by its higher R$^2$ score
and lower RMSE and MAE scores than other models.
As a result, the GP model is utilized in the MCMC sampling for IUQ.

\begin{figure}[!ht]
\begin{subfigure}{0.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{invuq/images/gp.png}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{invuq/images/nn.png}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{invuq/images/svr.png}
\end{subfigure}
\caption{
	Surrogate prediction vs actual test data for non-linear surrogates.
}
\label{fig:nonlin}
\end{figure}

\begin{table}[ht]
\centering
\caption{
	Comparison of surrogate performance on test data.
}
\label{tab:surrogates}
\begin{tabular}{lccc}
\toprule
Surrogate Model           & R$^2$   & RMSE  & MAE   \\
\midrule
Ordinary Least Squares    & 0.99708 & 0.253 & 0.174 \\
Lasso                     & 0.99706 & 0.255 & 0.174 \\
Ridge                     & 0.99708 & 0.253 & 0.174 \\
Gaussian Processes        & 0.99981 & 0.065 & 0.044 \\
Neural Network            & 0.99366 & 0.374 & 0.279 \\
Support Vector Regression & 0.99365 & 0.374 & 0.172 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{MCMC Sampling}

To get the posterior distributions of the calibration parameters,
the Metropolis-Hastings algorithm is used
to generate Markov Chain Monte Carlo (MCMC) samples
\cite{robert2004metropolis, van2018simple}.

Figure \ref{fig:trace} displays the trace plots of the MCMC samples
of the three fission-gas-behavior parameters.
Two chains of 100,000 samples are overlaid on the trace plots.
The cumulative averages of the blue and orange traces
are shown in black and red, respectively.
After an initial period, the cumulative averages of both chains converge.
The density plots of the parameter posterior samples
are shown in Figure \ref{fig:hist}.
Again, the density plots from both chains show similar trends.
The posterior of "dGrainHBS" is reminiscent of a Gaussian distribution,
whereas the posteriors of the other two parameters
are more akin to a uniform distribution.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{invuq/images/trace.png}
	\caption{
		Trace plots of the posterior samples
		of the fission-gas-behavior parameters from MCMC sampling.
	}
	\label{fig:trace}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{invuq/images/hist.png}
	\caption{
		Density plots of the posterior samples
		of the fission-gas-behavior parameters from MCMC sampling.
	}
	\label{fig:hist}
\end{figure}

The acceptance rate of the MCMC sampling was about 24\% for both chains,
which is close to the optimal value for mixing
for a random walk Metropolis algorithm.
To be sure of the convergence of the MCMC samples,
the Gelman-Rubin statistic and the effective sample size (ESS)
of the MCMC samples are also calculated and listed in Table \ref{tab:conv}.
The Gelman-Rubin statistic
is either 1 or very close to 1 for the posterior samples,
which indicates that no convergence issues were detected.
For all three parameters, the ESS is about 2000,
which is sufficient for stable estimates.
The MCMC samples were thus thinned by picking every 100th value
to reduce auto-correlation.
The remaining 2000 posterior samples are investigated for the IUQ process.

\begin{table}[ht]
\centering
\caption{
	Convergence diagnostics for the MCMC samples.
}
\label{tab:conv}
\begin{tabular}{lccc}
\toprule
Statistic              & dGrainHBS & FaceCovMax & SwellLink \\
\midrule
Effective Sample Size  & 2019      & 1878       & 2122      \\
Gelman-Rubin statistic & 1.0       & 1.0        & 1.001     \\
\bottomrule
\end{tabular}
\end{table}

\subsection{IUQ}

The diagonal subfigures in Figure \ref{fig:iuq}
shows the posterior distributions of the fission-gas-behavior parameters
after the IUQ process.
The distributions in this figure are from the thinned samples
unlike the distributions shown in Figure \ref{fig:hist}.
Thinning the samples did not change the characteristics of the distributions.
The means, standard deviations, and the 95\% credible intervals
of the posterior distributions are reported in Table \ref{tab:iuq}.

Besides the posteriors,
the correlations among the posteriors are also displayed
as contour plots in the off-diagonal of Figure \ref{fig:iuq}.
Even though the prior distributions of the parameters
are assumed to be independent,
Bayesian IUQ can still identify the correlations
among the parameter posteriors through MCMC sampling.
For instance, the posteriors of "dGrainHBS" and "FaceCovMax"
have a strong positive correlation ($\rho = 0.9$),
and the posteriors of "dGrainHBS" and "SwellLink"
have a weak positive correlation ($\rho = 0.25$).
Consequently, the correlation among the parameter posteriors
need to be taken into account when generating new samples.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{invuq/images/iuq.png}
	\caption{
		Posterior distributions of 3 fission-gas-behavior parameters.
		Marginal densities are in the diagonal
		and the pair-wise joint densities are in the off-diagonal.
	}
	\label{fig:iuq}
\end{figure}

\begin{table}[ht]
\centering
\caption{
	Posterior means, standard deviations, and 95\% credible intervals
	for all the fission-gas-behavior parameters.
}
\label{tab:iuq}
\begin{tabular}{lccc}
\toprule
Parameter  & Mean
		   & Std. deviation
		   & 95\% credible interval               \\
\midrule
dGrainHBS  & \num{3.72e-5}
		   & \num{3.42e-6}
		   & {[} \num{3.11e-5}, \num{4.37e-5} {]} \\
FaceCovMax & \num{7.57e-1}
		   & \num{8.82e-2}
		   & {[} \num{6.09e-1}, \num{8.99e-1} {]} \\
SwellLink  & \num{2.53e-2}
		   & \num{5.22e-3}
		   & {[} \num{1.66e-2}, \num{3.37e-2} {]} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{FUQ and Validation}

To validate the results from IUQ,
we propagated the posterior samples using two surrogate models: GP and Lasso.
The results from the forward propagation are shown in Figure \ref{fig:fuq}.
Both surrogate models predict
fuel swelling aligning closely with the experimental data,
even though IUQ was performed only with the GP surrogate.
This process does not lead to a proper validation of the IUQ results
since we are using the same surrogate for both IUQ and FUQ.
A more rigorous validation process would be generating new samples
from the joint posteriors of the parameters
and propagating them directly through DART.

\begin{figure}[!ht]
\begin{subfigure}{0.49\textwidth}
	\centering
	\caption{GP}
	\includegraphics[width=\textwidth]{invuq/images/fuq_gp.png}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
	\centering
	\caption{Lasso}
	\includegraphics[width=\textwidth]{invuq/images/fuq_las3.png}
\end{subfigure}
\caption{
	Forward propagation using (a) GP, and (b) Lasso surrogate models.
}
\label{fig:fuq}
\end{figure}

\section{Conclusions}

This work establishes a way
to perform IUQ on unknown fission-gas-behavior parameters of $\gamma$U-Mo
utilized in DART.
A dataset compiled from 3200 DART simulations are used for the study.
Only three parameters ("dGrainHBS", "FaceCovMax", and "SwellLink") out of seven
affects the fission gas swelling value in a significant way.
The other four parameters are inconsequential.
For surrogate modeling, GP with an RBF kernel is found to work the best.
MCMC sampling with this surrogate leads to
converged posterior probability distributions for the three parameters.
Forward propagations using these posteriors provide a way
to verify the whole process as well.
